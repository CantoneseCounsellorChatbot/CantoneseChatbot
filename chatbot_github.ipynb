{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot_github.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python383jvsc74a57bd0f6b2f86bbbe13c4b3d4ec962c48b23699d251edd071ca6768a784c865f7bbfa8",
      "display_name": "Python 3.8.3 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CantoneseCounsellorChatbot/CantoneseChatbot/blob/main/chatbot_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4ewHjaWJoqJ",
        "outputId": "520dc957-c5e0-4327-c097-1dc83362f65b"
      },
      "source": [
        "!pip install torch\n",
        "!pip install simpletransformers\n",
        "!pip install pytorch_pretrained_bert\n",
        "# !pip install sqlite3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: simpletransformers in /usr/local/lib/python3.7/dist-packages (0.61.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (4.61.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.1.5)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.10.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.1.95)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.7.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.4.1)\n",
            "Requirement already satisfied: transformers>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (4.6.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.82.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.10.31)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.19.5)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.22.2.post1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (1.24.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->simpletransformers) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->simpletransformers) (2.8.1)\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (2021.5.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (4.0.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.70.11.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (20.9)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.0.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers) (3.0.12)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.18.2)\n",
            "Requirement already satisfied: watchdog; platform_system != \"Darwin\" in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (2.1.2)\n",
            "Requirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (7.1.2)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (3.1.17)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.1.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (7.1.2)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (2.1.0)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.6.2)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (3.12.4)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (1.4)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (5.1.1)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.8.1)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.2.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (1.5.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (0.4.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (5.0.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (0.1.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (1.0.1)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (5.4.8)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (1.1.0)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (3.5.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (2.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->simpletransformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets->simpletransformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets->simpletransformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets->simpletransformers) (2.4.7)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit->simpletransformers) (4.4.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->streamlit->simpletransformers) (4.0.7)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.11.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.6.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.11.1)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (7.6.3)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.0.5)\n",
            "Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.5.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit->simpletransformers) (56.1.0)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->streamlit->simpletransformers) (4.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (2.0.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.8.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.7.1)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (22.0.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.10.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.3.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.4.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1)\n",
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.17.83)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.61.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.8.1+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.4.2)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.83 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.20.83)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.83->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.83->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement sqlite3 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for sqlite3\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GHgzgJ7dglG",
        "outputId": "ae8d3220-b54b-45d2-8e2c-245f535f4c27"
      },
      "source": [
        "!git clone https://github.com/CantoneseCounsellorChatbot/CantoneseChatbot.git\n",
        "# download question retrieval model\n",
        "!wget wget --header 'Host: doc-84-7c-drive-data-export.googleusercontent.com' --user-agent 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0' --header 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8' --header 'Accept-Language: zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2' --header 'Alt-Used: doc-84-7c-drive-data-export.googleusercontent.com' --header 'Cookie: AUTH_ge7bvq11gfcnqun4i11unf9jtd7k3grb_nonce=887ho9bhl0vi8' --header 'Upgrade-Insecure-Requests: 1' 'https://doc-84-7c-drive-data-export.googleusercontent.com/download/b0dk5h7sfcgnut9u18e035rlff1cmfut/0ugh68sfkevulagr1akgqjr6plkour4f/1622358000000/3d6f0bd8-8c6d-4e0d-8593-5bf9584053c5/110289539990330671686/ADt3v-OoylBS-nV_H-EKR7vKCUNYHHLJQ92h7i9619jhNvFrMZUrQwU06XJAv3K1I4nSkP_stFiKLKqendwj10SO0DeURqGrBmqeytvp3N7b-xiSd11P6ufPFyM9XkBkFzvbqBdBr8wduaN0HID0-c4oSvrrAvSCnQf9_R2bmuZzhk6sGAcL5_L7OfoIW78-L-c9-FXdL4PUoBYfAz5WIq6-sh6ys0vcVRoZJLXg56UzQx7OIJ6YRLdPmLWslIzMHUde7kQCh3hFhIR-MMHJLPD6rFHx_vCla0VwAqsmrl-OV7CDZEczuadlgEix_gvQT_JuGuEUOV1wpFOOuqjCiHm6b5fYKHLTcg==?authuser=0&nonce=887ho9bhl0vi8&user=110289539990330671686&hash=4a9eo89qti7mle382icqi2a5nh7m6bju' --output-document 'regression_question-20210530T080224Z-001.zip'\n",
        "!unzip -o -d CantoneseChatbot/ regression_question-20210530T080224Z-001.zip\n",
        "!rm -rf regression_question-20210530T080224Z-001.zip\n",
        "# download advice retrieval model\n",
        "wget --header 'Host: doc-6c-0c-drive-data-export.googleusercontent.com' --user-agent 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0' --header 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8' --header 'Accept-Language: zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2' --header 'Alt-Used: doc-6c-0c-drive-data-export.googleusercontent.com' --header 'Cookie: AUTH_ge7bvq11gfcnqun4i11unf9jtd7k3grb_nonce=hmj55acgd8nis' --header 'Upgrade-Insecure-Requests: 1' 'https://doc-6c-0c-drive-data-export.googleusercontent.com/download/b0dk5h7sfcgnut9u18e035rlff1cmfut/bq9kobimcv9gqmar5g2cor9m612a8hq2/1622358000000/b43e123d-c19b-400a-926d-ed288a2245ad/110289539990330671686/ADt3v-MjuWjNvjBGGfnvPrzSeJLxTuUKfWYm01LQxlaX6h02IIrbSc_sMAjYd6LIk9nwzffo_IEKDqxP9BKiyf0n85eqolU5MD9_GO0NkNa3bCia41t4XbU2ZveYvNmE58ri8yr9vQ4ViC7GcmBrOPK1BEAa-lfz717lYKwTiIr6vlLgn-HvB_CNIPkGOhIu5PUF8fee8_GtKgSkyPtLK0YfZbU_kO9u1uYD3yq55axvcEOmZIjNKR7YQpsDhI9bT0S95Lt9YIjmFNGgVer1OCQhlCidw93vQ_jFYWOmdCXUL8WH7lJbpI5mOg1v3Qsfo8hS3UPfyd1V?authuser=0&nonce=hmj55acgd8nis&user=110289539990330671686&hash=4q6qu0nlnnb6gtr8ofpkbtg4e6d7gn96' --output-document 'regression_advice-20210530T080205Z-001.zip'\n",
        "!unzip -o -d CantoneseChatbot/ regression_advice-20210530T080205Z-001.zip\n",
        "!rm -rf regression_advice-20210530T080205Z-001.zip\n",
        "# download restatement retrieval model\n",
        "wget --header 'Host: doc-4s-0o-drive-data-export.googleusercontent.com' --user-agent 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0' --header 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8' --header 'Accept-Language: zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2' --header 'Alt-Used: doc-4s-0o-drive-data-export.googleusercontent.com' --header 'Cookie: AUTH_ge7bvq11gfcnqun4i11unf9jtd7k3grb_nonce=6m8umefp60h92' --header 'Upgrade-Insecure-Requests: 1' 'https://doc-4s-0o-drive-data-export.googleusercontent.com/download/b0dk5h7sfcgnut9u18e035rlff1cmfut/8ulj7aaq7s5lgpkgm3a6h9251aqimoi7/1622358000000/4b6bbe1a-34f8-4d87-a520-f1f5630b46ea/110289539990330671686/ADt3v-NGoQiM242fqh4Gxbyn-ygueNPDhvLnpEzIXxN11lry7qpYoRi9bgUDJ0ixxQAccp69gFTd7JZge6fkYQdGqjFXlRJSDQ-T8aZtyqNh3AydbGe4Es2CbyS1WIO84w15ysXAE7Wk2vKofxXfkHs-aUQae9jLunoGThijd0DGmrqkTRddcKpZEAOoA1lm0UD19YF1hMkoWV7DEiiDo974VgZM2TDFXcdjU8iOSPl32ItIXQnWpS2TdlaY3_tApHFkKsKXloP2h_jlxLO3N_aXKb66Cghzyy2jiQsBV-0O4oJHsj2fTBnCmNEvHnrJQQcbPLd46HDz4QdXRWkSICyf9Lwowp7cIQ==?authuser=0&nonce=6m8umefp60h92&user=110289539990330671686&hash=f3dnoqtmjj3fkfihnaar2psurgoo63r8' --output-document 'regression_restatement-20210530T080229Z-001.zip'\n",
        "!unzip -o -d CantoneseChatbot/ regression_restatement-20210530T080229Z-001.zip\n",
        "!rm -rf regression_restatement-20210530T080229Z-001.zip\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: 目标路径 'CantoneseChatbot' 已经存在，并且不是一个空目录。\n",
            "--2021-05-30 16:31:30--  http://wget/\n",
            "正在解析主机 wget (wget)... 失败：未知的名称或服务。\n",
            "wget: 无法解析主机地址 “wget”\n",
            "--2021-05-30 16:31:30--  https://doc-84-7c-drive-data-export.googleusercontent.com/download/b0dk5h7sfcgnut9u18e035rlff1cmfut/0ugh68sfkevulagr1akgqjr6plkour4f/1622358000000/3d6f0bd8-8c6d-4e0d-8593-5bf9584053c5/110289539990330671686/ADt3v-OoylBS-nV_H-EKR7vKCUNYHHLJQ92h7i9619jhNvFrMZUrQwU06XJAv3K1I4nSkP_stFiKLKqendwj10SO0DeURqGrBmqeytvp3N7b-xiSd11P6ufPFyM9XkBkFzvbqBdBr8wduaN0HID0-c4oSvrrAvSCnQf9_R2bmuZzhk6sGAcL5_L7OfoIW78-L-c9-FXdL4PUoBYfAz5WIq6-sh6ys0vcVRoZJLXg56UzQx7OIJ6YRLdPmLWslIzMHUde7kQCh3hFhIR-MMHJLPD6rFHx_vCla0VwAqsmrl-OV7CDZEczuadlgEix_gvQT_JuGuEUOV1wpFOOuqjCiHm6b5fYKHLTcg==?authuser=0&nonce=887ho9bhl0vi8&user=110289539990330671686&hash=4a9eo89qti7mle382icqi2a5nh7m6bju\n",
            "正在解析主机 doc-84-7c-drive-data-export.googleusercontent.com (doc-84-7c-drive-data-export.googleusercontent.com)... 142.250.207.65, 2404:6800:4005:804::2001\n",
            "正在连接 doc-84-7c-drive-data-export.googleusercontent.com (doc-84-7c-drive-data-export.googleusercontent.com)|142.250.207.65|:443... 已连接。\n",
            "已发出 HTTP 请求，正在等待回应... 200 OK\n",
            "长度： 1027397304 (980M) [application/octet-stream]\n",
            "正在保存至: “regression_question-20210530T080224Z-001.zip”\n",
            "\n",
            "regression_question 100%[===================>] 979.80M  15.6MB/s    用时 71s   \n",
            "\n",
            "2021-05-30 16:32:42 (13.8 MB/s) - 已保存 “regression_question-20210530T080224Z-001.zip” [1027397304/1027397304])\n",
            "\n",
            "下载完毕 --2021-05-30 16:32:42--\n",
            "总用时：1m 12s\n",
            "下载了：1 个文件，1m 11s (13.8 MB/s) 中的 980M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgzQ4JnMvd5X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy6h3zuSdglI",
        "outputId": "64857c12-5f6b-48c4-f992-c465b2b29cc9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  regression_question-20210530T080224Z-001.zip\n",
            "  inflating: /home/kent/newhome/Gitfile/CantoneseChatbot/CantoneseChatbot/regression_question/bestmodel/vocab.txt  \n",
            "  inflating: /home/kent/newhome/Gitfile/CantoneseChatbot/CantoneseChatbot/regression_question/bestmodel/tokenizer_config.json  \n",
            "  inflating: /home/kent/newhome/Gitfile/CantoneseChatbot/CantoneseChatbot/regression_question/bestmodel/config.json  \n",
            "  inflating: /home/kent/newhome/Gitfile/CantoneseChatbot/CantoneseChatbot/regression_question/bestmodel/special_tokens_map.json  \n",
            "  inflating: /home/kent/newhome/Gitfile/CantoneseChatbot/CantoneseChatbot/regression_question/bestmodel/eval_results.txt  \n",
            "  inflating: /home/kent/newhome/Gitfile/CantoneseChatbot/CantoneseChatbot/regression_question/bestmodel/model_args.json  \n",
            "  inflating: /home/kent/newhome/Gitfile/CantoneseChatbot/CantoneseChatbot/regression_question/bestmodel/scheduler.pt  \n",
            "  inflating: /home/kent/newhome/Gitfile/CantoneseChatbot/CantoneseChatbot/regression_question/bestmodel/training_args.bin  \n",
            "  inflating: /home/kent/newhome/Gitfile/CantoneseChatbot/CantoneseChatbot/regression_question/bestmodel/optimizer.pt  \n",
            "  inflating: /home/kent/newhome/Gitfile/CantoneseChatbot/CantoneseChatbot/regression_question/bestmodel/pytorch_model.bin  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZqIpv_pKbPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b422ab-6ad1-44b0-9a29-04bbc7f21980"
      },
      "source": [
        "import sys\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# sys.path.append('/content/drive/My Drive/chatbot/BertSum')\n",
        "from datetime import time\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "# import sqlite3\n",
        "import re\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0PRuSqkW9Jw"
      },
      "source": [
        "def regressionReply(post,model,candidate):\n",
        "  # print(\"regression\")\n",
        "  tokenizer = BertTokenizer.from_pretrained(model)\n",
        "  model = BertForSequenceClassification.from_pretrained(model)\n",
        "  model.to('cuda')\n",
        "  data=pd.read_csv(candidate)\n",
        "  advice_list = data.advice.drop_duplicates().to_list()\n",
        "  text=post\n",
        "\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  for a in advice_list:\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                      text+\"[SEP]\"+a,                      # Sentence to encode.\n",
        "                      add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                      max_length = 50,           # Pad & truncate all sentences.\n",
        "                      pad_to_max_length = True,\n",
        "                      return_attention_mask = True,   # Construct attn. masks.\n",
        "                      return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                  )\n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  batch_size = 32\n",
        "  # print(\"chatbot:{}\".format(advice))\n",
        "  prediction_data = TensorDataset(input_ids, attention_masks)\n",
        "  prediction_sampler = SequentialSampler(prediction_data)\n",
        "  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "  \n",
        "  predictions  = []\n",
        "  print(\"regression2\")\n",
        "  # Predict \n",
        "  for batch in tqdm(prediction_dataloader):\n",
        "      batch = tuple(t.to(\"cuda\") for t in batch)\n",
        "      b_input_ids, b_input_mask = batch\n",
        "      with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "      logits = outputs[0]\n",
        "      logits = logits.detach().cpu().numpy().reshape(-1).tolist()\n",
        "  #     label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "      predictions+=logits\n",
        "  #   true_labels.append(label_ids)\n",
        "\n",
        "  return advice_list[np.argmax(predictions)], np.max(predictions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ9O-QMYEpev"
      },
      "source": [
        "def general(aa):\n",
        "    def getQAlist():\n",
        "        qaList = []\n",
        "        conn = pd.read_csv(\"/content/drive/My Drive/chatbot/keyword_list.csv\")\n",
        "        \n",
        "\n",
        "        for index,row in conn.iterrows():\n",
        "          if pd.notna(row[\"Q\"]):\n",
        "            tmp = {\"Q\":\"\"+row[\"Q\"],\"A\":\"\"+row[\"A\"]}\n",
        "            qaList.append(tmp)\n",
        "          else:\n",
        "            genreal_reply=row[\"A\"].split(\"|\")\n",
        "        return qaList,genreal_reply\n",
        "\n",
        "    def answer(say,seg):\n",
        "        if (say[0]==\"你\") and (say.find(\"唔\")>0):\n",
        "            msg = handleSpecial(say,seg)\n",
        "            if msg !=\"\":\n",
        "                return msg\n",
        "        return \"@eliza@ \" + getAnswer(say)\n",
        "    def getAnswer(say):\n",
        "        tmpList,general_reply=getQAlist()\n",
        "        results = analyzeSay(say, tmpList, general_reply)\n",
        "        \n",
        "        msg=results[1]\n",
        "        if msg !=\"\":\n",
        "            return msg\n",
        "        else:\n",
        "            return \"然後呢？@发生错误@\"\n",
        "\n",
        "    def analyzeSay(say, tmpList, general_reply):\n",
        "        patterns = []\n",
        "        for i in range(len(tmpList)):\n",
        "            qa = tmpList[i]\n",
        "            qList = qa[\"Q\"].split(\" | \")\n",
        "            aList = qa[\"A\"].split(\"|\")            \n",
        "            elizakeyword = []\n",
        "            for j in range(len(qList)):\n",
        "                qi = qList[j]\n",
        " \n",
        "\n",
        "\n",
        "                if say.find(qi) >-1:\n",
        "\n",
        "\n",
        "                    elizakeyword.append(qi)\n",
        "                    tt=handlePunc(say, qi)\n",
        "\n",
        "                    tail = getTail(tt, qi)\n",
        "\n",
        "                    replacedTail = tail.replace(\"我\", \"#\")\n",
        "                    replacedTail = replacedTail.replace(\"你\", \"我\")\n",
        "                    replacedTail = replacedTail.replace(\"#\", \"你\")\n",
        "                    msg = [tail, aList[np.random.randint(len(aList))].replace(\"*\", replacedTail)+\"$\"+qi+\"$\"]\n",
        "                    patterns.append(msg)\n",
        "        if patterns==[]:\n",
        "            patterns.append([say, general_reply[np.random.randint(len(general_reply))].replace(\"*\", say)+\"$\"+\"None\"+\"$\"])\n",
        "\n",
        "\n",
        "        return getRandomPattern(patterns)\n",
        "\n",
        "#             except:\n",
        "#                 print(i)\n",
        "\n",
        "\n",
        "    def getRandomPattern(patterns):\n",
        "        return patterns[np.random.randint(len(patterns))]\n",
        "#     def getTail(say, q):\n",
        "#         print(\"lbk\")\n",
        "#         r= re.compile(r\"(.*){}([^?.;]*)\".format(q))\n",
        "#         tmp = r.match(say)\n",
        "#         print(tmp)\n",
        "#         if tmp:\n",
        "#             return tmp[1]\n",
        "#         return \"\"\n",
        "    def getTail(say, q):\n",
        "        r= r\"(.*)({})([^?.;]*)\".format(q)\n",
        "        tmp = re.findall(r,say)\n",
        "        if tmp !=[] :\n",
        "            return tmp[0][2]\n",
        "        return \"\"\n",
        "\n",
        "    def handlePunc(say, keyword):\n",
        "        punct = [\",\", \"\\\\\\\\.\", \"!\", \"-\", \"\\\\\\\\?\", \"，\", \"！\", \"？\", \":\", \";\", \"；\", \"：\", \"。\", \"、\", \"…\"]\n",
        "        tmppunc=\"\".join(punct)\n",
        "        post = say.find(keyword)\n",
        "        if post == -1:\n",
        "            return say\n",
        "        r1=r\"[{}\\s](.*?{}.*?)[{}\\s]\".format(tmppunc,keyword,tmppunc)\n",
        "        r2=r\"[{}\\s]*(.*?{}.*?)[{}\\s]\".format(tmppunc,keyword,tmppunc)\n",
        "        r3=r\"[{}\\s](.*?{}.*?)[{}\\s]*\".format(tmppunc,keyword,tmppunc)\n",
        "        if re.findall(r1,say) !=[]:\n",
        "\n",
        "            return re.findall(r1,say)[0]\n",
        "        elif re.findall(r2,say) !=[]:\n",
        "\n",
        "            return re.findall(r2,say)[0]\n",
        "        elif re.findall(r3,say) !=[]:\n",
        "\n",
        "            return re.findall(r3,say)[0]\n",
        "        else:\n",
        "            return say\n",
        "    reply = answer(aa,aa)\n",
        "    return reply"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpfEGlzYzlyP"
      },
      "source": [
        "def chatbot(chatbot_params):\n",
        "  params_df=pd.DataFrame(chatbot_params).T.sort_values(by=[\"order\"])\n",
        "  while True:\n",
        "    text=input(\"input:\")\n",
        "    label=0\n",
        "    for index, row in params_df.iterrows():\n",
        "      if index == \"general\":\n",
        "        print(\"chatbot: {}\".format(general(text)))\n",
        "        break\n",
        "      elif index==\"advice\":\n",
        "        modelpath = \"/content/drive/MyDrive/chatbot/pretrain-model/regression_advice/bestmodel\"\n",
        "        advicepath= \"/content/drive/My Drive/chatbot/candidate/adviceall.csv\"\n",
        "        reply, score = regressionReply(text,modelpath,advicepath)\n",
        "        if score > row[\"Threshold\"]:\n",
        "          print(\"chatbot: {}\".format(reply))\n",
        "          break\n",
        "        continue\n",
        "      elif index==\"question\":\n",
        "        modelpath = \"/content/drive/MyDrive/chatbot/pretrain-model/regression_question/bestmodel\"\n",
        "        advicepath= \"/content/drive/MyDrive/chatbot/candidate/question.csv\"\n",
        "        reply, score = regressionReply(text,modelpath,advicepath)\n",
        "        if score > row[\"Threshold\"]:\n",
        "          print(\"chatbot: {}\".format(reply))\n",
        "          break\n",
        "        continue\n",
        "      elif index==\"restatement\":\n",
        "        modelpath = \"/content/drive/MyDrive/chatbot/pretrain-model/regression_restatement/bestmodel\"\n",
        "        advicepath= \"/content/drive/MyDrive/chatbot/candidate/restatement.csv\"\n",
        "        reply, score = regressionReply(text,modelpath,advicepath)\n",
        "        if score > row[\"Threshold\"]:\n",
        "          print(\"chatbot: {}\".format(reply))\n",
        "          break\n",
        "        continue\n",
        "      elif index==\"bertsum\":\n",
        "        print(\"chatbot: {}\".format(general(text)))\n",
        "        break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCdJCiYVkago"
      },
      "source": [
        "# The default response order is advice-->question-->restatement-->bertsum, you can change the order by changing the parameter b\n",
        "# The threshold of \"advice\",\"question\"and\"restatement\" ranges from 0 to 1.1\n",
        "# The threshold of \"bertsum\" ranges from -10 to 0\n",
        "chatbot_params={\"advice\":{\"Threshold\":1.0,\"order\":1},\n",
        "                \"question\":{\"Threshold\":1.0,\"order\":2},\n",
        "                \"restatement\":{\"Threshold\":1.0,\"order\":3},\n",
        "                \"bertsum\":{\"Threshold\":-2,\"order\":4},\n",
        "                \"general\":{\"Threshold\":None,\"order\":5}\n",
        "                }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhYFvarHoEmu"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "lOrZoU53JkEs",
        "outputId": "4f157d89-fd2c-4ccf-d177-68a6ee4e7acd"
      },
      "source": [
        "chatbot(chatbot_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:我覺得考試好難\n",
            "regression\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "  1%|          | 1/143 [00:00<00:18,  7.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "regression2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 143/143 [00:13<00:00, 10.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "chatbot: 係咪發生左啲咩事? 你冷靜啲先,可以搵學校嘅counsellor傾下，唔好俾咁大負能量自己\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m         \u001b[0;31m# flush i/o\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-b575470cc62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchatbot_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-f5c42bece27f>\u001b[0m in \u001b[0;36mchatbot\u001b[0;34m(chatbot_params)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mparams_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchatbot_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"order\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m                                       reply_content, parent, ident)\n\u001b[1;32m    703\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_is_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \"\"\"Override in subclasses to find completions.\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         self.session.send(stream, 'apply_reply', reply_content,\n\u001b[1;32m    736\u001b[0m                     parent=parent, ident=ident,buffers=result_buf, metadata=md)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wYZ2TNpzxclL",
        "outputId": "753fe7ba-8a5b-41bc-e2bc-57fc28b4a9b4"
      },
      "source": [
        "# BertSum, still have some bug\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "from tqdm import tqdm, trange\n",
        "import json\n",
        "\n",
        "from preprocess import LCSTSProcessor, convert_examples_to_features, create_dataset\n",
        "from model import BertAbsSum\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from utils import rouge\n",
        "from utils import convert_to_unicode\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, src, tgt=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            src: string. The untokenized text of the target sequence.\n",
        "            tgt: (Optional) string. The untokenized text of the target.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.src = src\n",
        "        self.tgt = tgt\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                    level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model_path\",\n",
        "                    default=None,\n",
        "                    type=str,\n",
        "                    required=True,\n",
        "                    help=\"The path to trained model.\")\n",
        "parser.add_argument(\"--config_path\",\n",
        "                    default=None,\n",
        "                    type=str,\n",
        "                    required=True,\n",
        "                    help=\"The path to config file.\")                    \n",
        "# parser.add_argument(\"--eval_path\",\n",
        "#                     default=None,\n",
        "#                     type=str,\n",
        "#                     required=True,\n",
        "#                     help=\"The path to the evaluation data. Should end with .tsv.\")\n",
        "parser.add_argument(\"--bert_model\", \n",
        "                    default=None, \n",
        "                    type=str, \n",
        "                    required=True,\n",
        "                    help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
        "                    \"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"\n",
        "                    \"bert-base-multilingual-cased, bert-base-chinese.\")\n",
        "# parser.add_argument(\"--result_path\",\n",
        "#                     default=None,\n",
        "#                     type=str,\n",
        "#                     required=True,\n",
        "#                     help=\"The path where you save your results.\")                    \n",
        "parser.add_argument(\"--max_src_len\",\n",
        "                    default=130,\n",
        "                    type=int,\n",
        "                    help=\"Max sequence length for source text. Sequences will be truncated or padded to this length\")\n",
        "parser.add_argument(\"--max_tgt_len\",\n",
        "                    default=30,\n",
        "                    type=int,\n",
        "                    help=\"Max sequence length for target text. Sequences will be truncated or padded to this length\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # args = parser.parse_args()\n",
        "    args = parser.parse_args(args=['--model_path','/content/drive/MyDrive/chatbot/pretrain-model/output_BTS_re+pr05190/model/BertAbsSum_14.bin',\n",
        "                                   '--config_path','/content/drive/MyDrive/chatbot/pretrain-model/output_BTS_re+pr05190/model/config.json',\n",
        "                                   '--bert_model','/content/drive/MyDrive/chatbot/BertSum/pretrained_model/bert-base-chinese'])\n",
        "\n",
        "\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    device=torch.device(\"cpu\")\n",
        "    with open(args.config_path, 'r') as f:\n",
        "        config = json.load(f)\n",
        "    model = BertAbsSum(args.bert_model, config['decoder_config'], device)\n",
        "    model.load_state_dict(torch.load(args.model_path))\n",
        "    model.to(device)\n",
        "\n",
        "    processor = LCSTSProcessor()\n",
        "    tokenizer = BertTokenizer.from_pretrained(os.path.join(args.bert_model, 'vocab.txt'))\n",
        "    # test_examples = processor.get_examples(args.eval_path)\n",
        "    post=input(\"input:\")\n",
        "    test_examples = [InputExample(guid=0,src=convert_to_unicode(post),tgt=convert_to_unicode(post)),InputExample(guid=1,src=convert_to_unicode(post),tgt=convert_to_unicode(post))]\n",
        "    test_features = convert_examples_to_features(test_examples, args.max_src_len, args.max_tgt_len, tokenizer)\n",
        "    test_data = create_dataset(test_features)\n",
        "\n",
        "    test_sampler = SequentialSampler(test_data)\n",
        "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE, drop_last=True)\n",
        "\n",
        "    model.eval()\n",
        "    # if not os.path.exists(args.result_path):\n",
        "    #     os.mkdir(args.result_path)\n",
        "    # f_log = open(os.path.join(args.result_path, 'log.txt'), 'w', encoding='utf-8')\n",
        "\n",
        "    hyp_list = []\n",
        "    ref_list = []\n",
        "    for batch in tqdm(test_dataloader, desc=\"Iteration\"):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        pred, beam_score = model.beam_decode(batch[0], batch[1], 3, 3)\n",
        "        src, tgt = batch[0], batch[2]\n",
        "        for i in range(BATCH_SIZE):\n",
        "            sample_src = \"\".join(tokenizer.convert_ids_to_tokens(src[i].cpu().numpy())).split('[CLS]')[1].split('[SEP]')[0] + '\\n'\n",
        "            sample_tgt = \"\".join(tokenizer.convert_ids_to_tokens(tgt[i].cpu().numpy())).split('[CLS]')[1].split('[SEP]')[0] + '\\n'\n",
        "            sample_pred = \"\".join(tokenizer.convert_ids_to_tokens(pred[i][0])).split('[SEP]')[0] + '\\n'\n",
        "            sample_pred_2 = \"\".join(tokenizer.convert_ids_to_tokens(pred[i][1])).split('[SEP]')[0] + '\\n'\n",
        "            sample_pred_3 = \"\".join(tokenizer.convert_ids_to_tokens(pred[i][2])).split('[SEP]')[0] + '\\n'\n",
        "\n",
        "            print('Hypothesis: ' + sample_pred)\n",
        "\n",
        "\n",
        "\n",
        "            print('score: ' + str(beam_score[i]))\n",
        "            \n",
        "            # f_hyp.write(sample_pred)\n",
        "            # f_ref.write(sample_tgt)\n",
        "\n",
        "\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/25/2021 15:35:04 - INFO - pytorch_pretrained_bert.modeling -   loading archive file /content/drive/MyDrive/chatbot/BertSum/pretrained_model/bert-base-chinese\n",
            "05/25/2021 15:35:04 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "05/25/2021 15:35:11 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file /content/drive/MyDrive/chatbot/BertSum/pretrained_model/bert-base-chinese/vocab.txt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "input:hello\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "examples: 100%|██████████| 2/2 [00:00<00:00, 5611.11it/s]\n",
            "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "lbk\n",
            "[tensor([0.0775, 0.0794, 0.3002])]\n",
            "tensor(0.0775)\n",
            "lbk\n",
            "[tensor([0.0775, 0.0794, 0.3002])]\n",
            "tensor(0.0794)\n",
            "lbk\n",
            "[tensor([0.0775, 0.0794, 0.3002])]\n",
            "tensor(0.3002)\n",
            "lbk\n",
            "[tensor([0.0775, 0.0794, 0.3002]), tensor([0.0662, 0.0929, 0.0834])]\n",
            "tensor(0.0662)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1c8c89da8b9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Iteration\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/chatbot/BertSum/model.py\u001b[0m in \u001b[0;36mbeam_decode\u001b[0;34m(self, src_seq, src_mask, beam_size, n_best)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 active_inst_idx_list = beam_decode_step(\n\u001b[0;32m--> 201\u001b[0;31m                     inst_dec_beams, len_dec_seq, src_seq, src_enc, inst_idx_to_position_map, beam_size)\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mactive_inst_idx_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/chatbot/BertSum/model.py\u001b[0m in \u001b[0;36mbeam_decode_step\u001b[0;34m(inst_dec_beams, len_dec_seq, src_seq, enc_output, inst_idx_to_position_map, beam_size)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mn_active_inst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst_idx_to_position_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mdec_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_beam_dec_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst_dec_beams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_dec_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mword_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_active_inst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/chatbot/BertSum/model.py\u001b[0m in \u001b[0;36mprepare_beam_dec_seq\u001b[0;34m(inst_dec_beams, len_dec_seq)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mprepare_beam_dec_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst_dec_beams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_dec_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0mdec_partial_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minst_dec_beams\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m                 \u001b[0mdec_partial_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_partial_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mdec_partial_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_partial_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_dec_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/chatbot/BertSum/model.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mprepare_beam_dec_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst_dec_beams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_dec_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0mdec_partial_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minst_dec_beams\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m                 \u001b[0mdec_partial_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_partial_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mdec_partial_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_partial_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_dec_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/chatbot/BertSum/transformer/Beam.py\u001b[0m in \u001b[0;36mget_current_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_current_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;34m\"Get the outputs for the current timestep.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tentative_hypothesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_current_origin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/chatbot/BertSum/transformer/Beam.py\u001b[0m in \u001b[0;36mget_tentative_hypothesis\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mhyps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hypothesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mhyps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mConstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBOS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhyps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mdec_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/chatbot/BertSum/transformer/Beam.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mhyps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hypothesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mhyps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mConstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBOS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhyps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mdec_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/chatbot/BertSum/transformer/Beam.py\u001b[0m in \u001b[0;36mget_hypothesis\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_ks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_ks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mhyp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_ys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_ks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kONvzI1qIIZ6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z1kAi-xf-Lc",
        "outputId": "a9283eed-102d-409b-8449-454ee1c35856"
      },
      "source": [
        "import numpy as np\n",
        "np.random.randint(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BuueSKUgAgU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfeZ2vHndLbx"
      },
      "source": [
        "def chatbot(text=None,adviceThreshold=1.0,questionThreshold=1.0,restatement_Threshold=1.0,bertsumThreshold=-2):\n",
        "  while True:\n",
        "    if text ==None:\n",
        "      text=input(\"input:\")\n",
        "    advice, score = regressionReply(text,\"/content/drive/MyDrive/chatbot/pretrain-model/regression_advice/bestmodel\",\"/content/drive/My Drive/chatbot/candidate/adviceall.csv\")\n",
        "    if score >adviceThreshold:\n",
        "      print(\"chatbot: {}\".format(advice))\n",
        "      continue\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}